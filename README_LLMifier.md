# ğŸ½ï¸ LLMifier â€” Recettes selon la mÃ©tÃ©o

## 1. PrÃ©sentation du projet

Cette application **Java console** propose automatiquement un **menu complet** (entrÃ©e, plat, dessert) en fonction :

- de la **mÃ©tÃ©o actuelle** (via lâ€™API *OpenWeatherMap*),
- de la **saison** (dÃ©duite de la date courante),
- des **prÃ©fÃ©rences alimentaires de lâ€™utilisateur**, fournies sous forme dâ€™historique,
- en sâ€™appuyant sur un **LLM local exÃ©cutÃ© via Ollama**.

Lâ€™objectif est de montrer comment intÃ©grer un **modÃ¨le de langage** dans une application Java, tout en tenant compte de **donnÃ©es contextuelles rÃ©elles**.

---

## 2. LLM testÃ©s et comparaison

Dans le cadre de la fonction `sample1()` (calcul simple) ainsi que pour la gÃ©nÃ©ration de menus, plusieurs modÃ¨les ont Ã©tÃ© testÃ©s.

TinyLlama â€” tinyllama-1.1B
CritÃ¨re	 : Valeur
Taille : 	~1.1 milliard de paramÃ¨tres
Poids disque : 	~600â€“700 Mo (selon quantization)
Ã‰nergie consommÃ©e : 	Faible
Vitesse	 : TrÃ¨s rapide
QualitÃ© observÃ©e : 	Faible

Observations :
- rÃ©ponses souvent imprÃ©cises,
- erreurs frÃ©quentes sur des calculs simples,
- non-respect des contraintes (prÃ©fÃ©rences alimentaires, format imposÃ©),
- rÃ©sultats instables et peu fiables.

Conclusion :
TinyLlama est rapide et lÃ©ger, mais la qualitÃ© est insuffisante pour une application nÃ©cessitant prÃ©cision et respect strict des consignes.
Il nâ€™est pas adaptÃ© Ã  ce projet.
---


### ğŸ”¹ Phi-4 â€” `phi-4` / `phi-4-mini`
CritÃ¨re : 	Valeur
Taille : ~3.8 milliards de paramÃ¨tres
Poids disque : 	~2 Ã  3 Go
Ã‰nergie consommÃ©e : ModÃ©rÃ©e
Vitesse : Rapide
QualitÃ© observÃ©e :	Moyenne Ã  bonne

Observations :
- rÃ©sultats corrects sur des calculs simples,
- rÃ©ponses globalement cohÃ©rentes,
- justifications intÃ©ressantes,
- mais non-respect frÃ©quent des prÃ©fÃ©rences utilisateur (ex : ingrÃ©dients interdits),
- parfois incomplet (entrÃ©e ou dessert manquant).

Conclusion :
Phi-4 constitue un bon compromis technique, mais son manque de rigueur dans le respect des contraintes limite son utilisation pour ce projet.


### GPT-OSS â€” `gpt-oss:120b-cloud`
CritÃ¨re	 : Valeur
Taille : 	~120 milliards de paramÃ¨tres
Poids disque : 	TrÃ¨s Ã©levÃ© (exÃ©cution cloud)
Ã‰nergie consommÃ©e  :	TrÃ¨s importante
Vitesse	: Moyenne
QualitÃ© observÃ©e : 	Excellente

Observations :
- rÃ©ponses trÃ¨s cohÃ©rentes et structurÃ©es
- respect strict du format demandÃ© (entrÃ©e / plat / dessert),
- excellente prise en compte des prÃ©fÃ©rences utilisateur,
- justifications claires et pertinentes,
- trÃ¨s bonne comprÃ©hension du contexte mÃ©tÃ©o et saisonnier.

Conclusion :
MÃªme si GPT-OSS est trÃ¨s volumineux et Ã©nergivore, il est de loin le modÃ¨le le plus performant testÃ© dans ce projet.
Il fournit les rÃ©ponses les plus fiables, complÃ¨tes et pertinentes.
---

## 3. ParamÃ©trage de la tempÃ©rature

La **tempÃ©rature** est un paramÃ¨tre contrÃ´lant le degrÃ© de **crÃ©ativitÃ©** du modÃ¨le.

- `temperature = 0.0` â†’ rÃ©ponses dÃ©terministes (calculs, faits exacts)
- `temperature â‰ˆ 0.6 â€“ 0.8` â†’ rÃ©ponses naturelles pour la gÃ©nÃ©ration de menus
- `temperature > 1.0` â†’ crÃ©ativitÃ© accrue mais risque dâ€™erreurs

Dans ce projet :
- une **tempÃ©rature basse** est utilisÃ©e pour `sample1()` (calcul),
- une **tempÃ©rature modÃ©rÃ©e** est utilisÃ©e pour les recettes.

---

## 4. RÃ©ponses aux questions thÃ©oriques

### Quâ€™est-ce que lâ€™attention ?

Lâ€™attention est un mÃ©canisme qui permet au modÃ¨le de se concentrer sur les parties les plus pertinentes dâ€™une sÃ©quence dâ€™entrÃ©e.  
Elle attribue un poids aux mots (tokens) afin de mieux capturer le contexte et les relations entre eux.

---

### Quâ€™est-ce quâ€™un encodeur et un dÃ©codeur ?  
**A-t-on toujours besoin des deux ?**

- Lâ€™**encodeur** transforme lâ€™entrÃ©e en une reprÃ©sentation interne.
- Le **dÃ©codeur** gÃ©nÃ¨re la sortie Ã  partir de cette reprÃ©sentation.

Les modÃ¨les modernes de type GPT utilisent uniquement un **dÃ©codeur**, ce qui est suffisant pour la gÃ©nÃ©ration de texte.  
On nâ€™a donc **pas toujours besoin des deux**.

---

### Quâ€™est-ce que la tempÃ©rature ?

La tempÃ©rature modifie la distribution de probabilitÃ© lors de la gÃ©nÃ©ration des tokens :

- basse tempÃ©rature â†’ rÃ©ponses prÃ©cises et stables,
- haute tempÃ©rature â†’ rÃ©ponses plus variÃ©es mais moins fiables.

Câ€™est un compromis entre **exactitude** et **crÃ©ativitÃ©**.

---

## 5. Conclusion

Ce projet met en Ã©vidence lâ€™importance du **choix du modÃ¨le de langage** :

- les modÃ¨les trop petits manquent de prÃ©cision,
- les modÃ¨les trop grands sont Ã©nergivores et peu adaptÃ©s,
- un modÃ¨le intermÃ©diaire comme **Phi-3** offre un excellent compromis.

Lâ€™intÃ©gration dâ€™un LLM local avec des donnÃ©es mÃ©tÃ©o rÃ©elles permet de produire des rÃ©ponses **contextualisÃ©es, pertinentes et personnalisÃ©es**.
